{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9723a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 14:29:06.848601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imsave\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss\n",
    "import segmentation_models_3D as sm\n",
    "# segmentation_models = '/exp1/experiment1Softmax/segmentation_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6b4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(segmentation_models)\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc89ed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2563.422310464084\n",
      "[0 1 2 4]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATASET_PATH = '/data'\n",
    "#VALIDATION_DATASET_PATH = 'BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n",
    "\n",
    "sample_filename = '/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_FLAIR.nii'\n",
    "sample_filename2 = '/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_T1.nii'\n",
    "sample_filename3 = '/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_T2.nii'\n",
    "sample_filename4 = '/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_T1c.nii'\n",
    "sample_filename_mask = '/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_tumor_segmentation.nii'\n",
    "\n",
    "test_image_flair=nib.load(sample_filename).get_fdata()\n",
    "print(test_image_flair.max())\n",
    "#Scalers are applied to 1D so let us reshape and then reshape back to original shape. \n",
    "# test_image_flair=scaler.fit_transform(test_image_flair.reshape(-1, test_image_flair.shape[-1])).reshape(test_image_flair.shape)\n",
    "\n",
    "\n",
    "test_image_t1=nib.load('/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_T1.nii').get_fdata()\n",
    "# test_image_t1=scaler.fit_transform(test_image_t1.reshape(-1, test_image_t1.shape[-1])).reshape(test_image_t1.shape)\n",
    "\n",
    "test_image_t1ce=nib.load('/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_T1c.nii').get_fdata()\n",
    "# test_image_t1ce=scaler.fit_transform(test_image_t1ce.reshape(-1, test_image_t1ce.shape[-1])).reshape(test_image_t1ce.shape)\n",
    "\n",
    "test_image_t2=nib.load('/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_T2.nii').get_fdata()\n",
    "# test_image_t2=scaler.fit_transform(test_image_t2.reshape(-1, test_image_t2.shape[-1])).reshape(test_image_t2.shape)\n",
    "\n",
    "test_mask=nib.load('/data/UCSF-PDGM-0014_nifti/UCSF-PDGM-0014_tumor_segmentation.nii').get_fdata()\n",
    "test_mask=test_mask.astype(np.uint8)\n",
    "\n",
    "print(np.unique(test_mask))  #0, 1, 2, 4 (Need to reencode to 0, 1, 2, 3)\n",
    "test_mask[test_mask==4] = 3  #Reassign mask values 4 to 3\n",
    "print(np.unique(test_mask)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1029ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Paths:\n",
      "['/data/UCSF-PDGM-0416_nifti', '/data/UCSF-PDGM-0207_nifti', '/data/UCSF-PDGM-0285_nifti', '/data/UCSF-PDGM-0108_nifti', '/data/UCSF-PDGM-0409_FU001d_nifti']\n",
      "\n",
      "Validation Dataset Paths:\n",
      "['/data/UCSF-PDGM-0430_nifti', '/data/UCSF-PDGM-0190_nifti', '/data/UCSF-PDGM-0317_nifti', '/data/UCSF-PDGM-0027_nifti', '/data/UCSF-PDGM-0032_nifti']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Directory containing your data\n",
    "data_directory = '/data'\n",
    "\n",
    "# Exclude the specific path you want to skip\n",
    "excluded_path = '/data/UCSF-PDGM-0541_nifti'\n",
    "excluded_dirname = os.path.basename(excluded_path)\n",
    "if excluded_path in data_directory:\n",
    "    data_directory = data_directory.replace(excluded_path, '')\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(data_directory)\n",
    "\n",
    "# Exclude the specific path you want to skip\n",
    "# excluded_path = '/data/UCSF-PDGM-0541_nifti'\n",
    "# if excluded_path in all_files:\n",
    "#     all_files.remove(excluded_path)\n",
    "\n",
    "# Shuffle the list of files\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# Define the number of files for training and validation\n",
    "num_train_files = 400\n",
    "\n",
    "# Split the shuffled list into training and validation sets\n",
    "train_files = all_files[:num_train_files]\n",
    "val_files = all_files[num_train_files:]\n",
    "\n",
    "# Define the paths for training and validation datasets\n",
    "TRAIN_DATASET_PATH = [os.path.join(data_directory, file) for file in train_files]\n",
    "VAL_DATASET_PATH = [os.path.join(data_directory, file) for file in val_files]\n",
    "\n",
    "# Print the first few file paths for verification\n",
    "print(\"Training Dataset Paths:\")\n",
    "print(TRAIN_DATASET_PATH[:5])  # Print the first 5 paths\n",
    "print(\"\\nValidation Dataset Paths:\")\n",
    "print(VAL_DATASET_PATH[:5])    # Print the first 5 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7b3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t2_list = []\n",
    "train_t1ce_list = []\n",
    "train_flair_list = []\n",
    "train_mask_list = []\n",
    "\n",
    "# Iterate through all directories in TRAIN_DATASET_PATH\n",
    "for directory in TRAIN_DATASET_PATH:\n",
    "    # Extract the base directory name, removing '_nifti' if it is part of the directory name\n",
    "    base_directory_name = os.path.basename(directory)\n",
    "    \n",
    "    # Assuming the ID is correctly represented in the directory name without '_nifti'\n",
    "    ID = base_directory_name.replace('_nifti', '')\n",
    "\n",
    "    # Construct file paths without '_nifti' in the filename part\n",
    "    t2_path = glob.glob(os.path.join(directory, f'{ID}_T2.nii'))\n",
    "    t1ce_path = glob.glob(os.path.join(directory, f'{ID}_T1c.nii'))\n",
    "    flair_path = glob.glob(os.path.join(directory, f'{ID}_FLAIR.nii'))\n",
    "    mask_path = glob.glob(os.path.join(directory, f'{ID}_tumor_segmentation.nii'))\n",
    "\n",
    "    # Check if all modalities are present for the ID and add them at the same index\n",
    "    if t2_path and t1ce_path and flair_path and mask_path:\n",
    "        train_t2_list.append(t2_path[0])  \n",
    "        train_t1ce_list.append(t1ce_path[0])\n",
    "        train_flair_list.append(flair_path[0])\n",
    "        train_mask_list.append(mask_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a6a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Initialize lists for different types of files\n",
    "val_t2_list = []\n",
    "val_t1ce_list = []\n",
    "val_flair_list = []\n",
    "val_mask_list = []\n",
    "\n",
    "# Iterate through all directories in TRAIN_DATASET_PATH\n",
    "for directory in VAL_DATASET_PATH:\n",
    "    # Extract the base directory name, removing '_nifti' if it is part of the directory name\n",
    "    base_directory_name = os.path.basename(directory)\n",
    "    \n",
    "    # Assuming the ID is correctly represented in the directory name without '_nifti'\n",
    "    ID = base_directory_name.replace('_nifti', '')\n",
    "\n",
    "    # Construct file paths without '_nifti' in the filename part\n",
    "    t2_path = glob.glob(os.path.join(directory, f'{ID}_T2.nii'))\n",
    "    t1ce_path = glob.glob(os.path.join(directory, f'{ID}_T1c.nii'))\n",
    "    flair_path = glob.glob(os.path.join(directory, f'{ID}_FLAIR.nii'))\n",
    "    mask_path = glob.glob(os.path.join(directory, f'{ID}_tumor_segmentation.nii'))\n",
    "\n",
    "    # Check if all modalities are present for the ID and add them at the same index\n",
    "    if t2_path and t1ce_path and flair_path and mask_path:\n",
    "        val_t2_list.append(t2_path[0])  \n",
    "        val_t1ce_list.append(t1ce_path[0])\n",
    "        val_flair_list.append(flair_path[0])\n",
    "        val_mask_list.append(mask_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b2f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files at index 0:\n",
      "T2: /data/UCSF-PDGM-0430_nifti/UCSF-PDGM-0430_T2.nii\n",
      "T1ce: /data/UCSF-PDGM-0430_nifti/UCSF-PDGM-0430_T1c.nii\n",
      "FLAIR: /data/UCSF-PDGM-0430_nifti/UCSF-PDGM-0430_FLAIR.nii\n",
      "Mask: /data/UCSF-PDGM-0430_nifti/UCSF-PDGM-0430_tumor_segmentation.nii\n"
     ]
    }
   ],
   "source": [
    "index_to_check = 0  # Example index\n",
    "if index_to_check < len(val_t2_list):\n",
    "    print(f\"Files at index {index_to_check}:\")\n",
    "    print(f\"T2: {val_t2_list[index_to_check]}\")\n",
    "    print(f\"T1ce: {val_t1ce_list[index_to_check]}\")\n",
    "    print(f\"FLAIR: {val_flair_list[index_to_check]}\")\n",
    "    print(f\"Mask: {val_mask_list[index_to_check]}\")\n",
    "else:\n",
    "    print(\"Index is out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb11062",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = '/exp1/experiment1Softmax/images'\n",
    "masks_dir = '/exp1/experiment1Softmax/masks'\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(masks_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eddd466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = '/exp1/experiment1Softmax/val/images'\n",
    "masks_dir = '/exp1/experiment1Softmax/val/masks'\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(masks_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e0aa921",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = '/exp1/savedModels'\n",
    "# masks_dir = '/exp1/new_model/experiment1Softmax/val/masks'\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(models, exist_ok=True)\n",
    "# os.makedirs(masks_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a357769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_dir, img_list):\n",
    "    images = []\n",
    "    for image_name in img_list:    \n",
    "        if image_name.endswith('.npy'):\n",
    "            image_path = os.path.join(img_dir, image_name)\n",
    "            image = np.load(image_path)\n",
    "            images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n",
    "    L = len(img_list)\n",
    "\n",
    "    while True:  # Generator loop\n",
    "        for batch_start in range(0, L, batch_size):\n",
    "            limit = min(batch_start + batch_size, L)\n",
    "\n",
    "            X = load_img(img_dir, img_list[batch_start:limit])\n",
    "            Y = load_img(mask_dir, mask_list[batch_start:limit])\n",
    "\n",
    "            yield (X, Y)  # Yielding a tuple with two elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb02a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "train_img_dir = \"/exp1/experiment1Softmax/images/\"\n",
    "train_mask_dir = \"/exp1/experiment1Softmax/masks/\"\n",
    "train_img_list = sorted(os.listdir(train_img_dir))\n",
    "train_mask_list = sorted(os.listdir(train_mask_dir))\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_img_datagen = imageLoader(train_img_dir, train_img_list, train_mask_dir, train_mask_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
