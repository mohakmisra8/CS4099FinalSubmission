{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data as torch_data\n",
    "from sklearn import model_selection as sk_model_selection\n",
    "from torch.nn import functional as torch_functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio numpy pandas pydicom opencv-python-headless matplotlib seaborn tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install efficientnet_pytorch_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch3dpath = \"EfficientNet-PyTorch-3D\"\n",
    "# pytorch3dpath = \"3DAutoencoder/Transformer/EfficientNet-PyTorch-3D\"\n",
    "# einops = \"/home/mm510/Dissertation/3DAutoencoder/Transformer/einops\"\n",
    "# einops = \"3DAutoencoder/Transformer/einops\"\n",
    "\n",
    "# data_directory = \"/data/mm510/UCSF-PDGM-v3/\"\n",
    "data_directory = \"/data/\"\n",
    "csv_directory = \"UCSF-PDGM-metadata_v2.csv\"\n",
    "# csv_directory = \"UCSF-PDGM-metadata_v2.csv\"\n",
    "\n",
    "sys.path.append(pytorch3dpath)\n",
    "# sys.path.append(einops)\n",
    "\n",
    "from efficientnet_pytorch_3d import EfficientNet3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mri_types = ['_FLAIR','_T1','_T1c','_T2', 'all']\n",
    "# mri_types = ['_FLAIR']\n",
    "SIZE = 256\n",
    "NUM_IMAGES = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_existent_paths(csv_file, output_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # List to hold indices of rows to be deleted\n",
    "    rows_to_delete = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract ID and pad it\n",
    "        id_num = row['ID'].split('-')[-1].zfill(4)\n",
    "\n",
    "        # Construct the path\n",
    "#         path = f\"/home/mm510/data/UCSF-PDGM-v3/UCSF-PDGM-{id_num}_nifti\"\n",
    "        path = f\"/data/UCSF-PDGM-{id_num}_nifti\"\n",
    "\n",
    "        # Check if path exists\n",
    "        if not os.path.exists(path):\n",
    "            # Mark the row index for deletion\n",
    "            rows_to_delete.append(index)\n",
    "\n",
    "    # Drop the rows where the path does not exist\n",
    "    df = df.drop(rows_to_delete)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/home/mm510/Dissertation/3DAutoencoder/Transformer/modified_train_data.csv'  \n",
    "# output_file = '/home/mm510/Dissertation/3DAutoencoder/Transformer/filtered_output_with_no_non_existant_paths.csv'  \n",
    "csv_file = 'modified_train_data.csv'  \n",
    "output_file = 'filtered_output_with_no_non_existant_paths.csv' \n",
    "filter_non_existent_paths(csv_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def extractIDwithoutNifTi(path):\n",
    "    base_name = os.path.basename(path)\n",
    "    id_without_nifti = base_name.replace('_nifti', '')\n",
    "    return id_without_nifti\n",
    "\n",
    "# img_size=(64, 96, 96)\n",
    "def load_nii_image(path, img_size=(128, 128, 128), rotate=0):\n",
    "    nii = nib.load(path)\n",
    "    data = nii.get_fdata()\n",
    "\n",
    "    if rotate > 0:\n",
    "        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n",
    "        if len(data.shape) == 3:\n",
    "            data = np.array([cv2.rotate(slice, rot_choices[rotate]) for slice in data])\n",
    "        else:\n",
    "            data = cv2.rotate(data, rot_choices[rotate])\n",
    "    \n",
    "    if len(data.shape) == 3:\n",
    "        data = np.array([cv2.resize(slice, (img_size[1], img_size[2]), interpolation=cv2.INTER_AREA) for slice in data])\n",
    "    else:\n",
    "        data = cv2.resize(data, (img_size[1], img_size[2]), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Resize depth if necessary\n",
    "    if data.shape[0] != img_size[0]:\n",
    "        new_data = np.zeros((img_size[0], img_size[1], img_size[2]))\n",
    "        depth = min(img_size[0], data.shape[0])\n",
    "        new_data[:depth] = data[:depth]\n",
    "        data = new_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_nii_images_3d(path, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"_FLAIR\", split=\"train\", rotate=0):\n",
    "    # Check if the path exists\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "        return None  # or handle the non-existent path as needed\n",
    "\n",
    "    files = sorted(glob.glob(path))\n",
    "    # print(\"Files found:\", files)\n",
    "\n",
    "    middle = len(files) // 2\n",
    "    num_imgs2 = num_imgs // 2\n",
    "    p1 = max(0, middle - num_imgs2)\n",
    "    p2 = min(len(files), middle + num_imgs2)\n",
    "\n",
    "    if not files[p1:p2]:\n",
    "        raise ValueError(f\"No files to process in the specified range. Path: {path}\")\n",
    "\n",
    "    # img3d = np.stack([load_nii_image(print(f\"Processing file: {f}\") or f, img_size=img_size, rotate=rotate) for f in files[p1:p2]])\n",
    "#     img3d = np.stack([load_nii_image(f, img_size=(64, 96, 96), rotate=rotate) for f in files[p1:p2]])\n",
    "    img3d = np.stack([load_nii_image(f, img_size=(128, 128, 128), rotate=rotate) for f in files[p1:p2]])\n",
    "\n",
    "\n",
    "    if img3d.shape[-1] < num_imgs:\n",
    "        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n",
    "        img3d = np.concatenate((img3d, n_zero), axis=-1)\n",
    "        \n",
    "    if np.min(img3d) < np.max(img3d):\n",
    "        img3d = img3d - np.min(img3d)\n",
    "        img3d = img3d / np.max(img3d)\n",
    "            \n",
    "    return np.expand_dims(img3d, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 128, 128, 128)\n",
      "0.0 1.0 0.07978620511033317 0.0\n"
     ]
    }
   ],
   "source": [
    "a = load_nii_images_3d(\"/data/UCSF-PDGM-0004_nifti/UCSF-PDGM-0004_FLAIR.nii\")\n",
    "print(a.shape)\n",
    "print(np.min(a), np.max(a), np.mean(a), np.median(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCSF-PDGM-0004\n"
     ]
    }
   ],
   "source": [
    "print(extractIDwithoutNifTi(\"/data/UCSF-PDGM-0004_nifti\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('UCSF-PDGM-metadata_v2.csv')\n",
    "\n",
    "# Remove rows with null values in \"MGMT status\" column\n",
    "df = df.dropna(subset=['MGMT status'])\n",
    "\n",
    "# Convert the \"MGMT status\" column with updated mapping (indeterminate to NaN for removal)\n",
    "status_mapping = {'negative': 0, 'positive': 1, 'indeterminate': None}\n",
    "df['MGMT status'] = df['MGMT status'].map(status_mapping)\n",
    "\n",
    "# Now remove rows where \"MGMT status\" became None due to mapping (formerly indeterminate)\n",
    "df = df.dropna(subset=['MGMT status'])\n",
    "\n",
    "# Convert \"MGMT status\" to integer to ensure values are 0 and 1, not 0.0 and 1.0\n",
    "df['MGMT status'] = df['MGMT status'].astype(int)\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "output_path = 'filtered_output.csv'\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'MGMT status': [0 1]\n",
      "Number of null values in 'MGMT status': 0\n"
     ]
    }
   ],
   "source": [
    "unique_values = df['MGMT status'].unique()\n",
    "print(\"Unique values in 'MGMT status':\", unique_values)\n",
    "\n",
    "# Check for null values in the \"MGMT status\" column\n",
    "null_count = df['MGMT status'].isnull().sum()\n",
    "print(\"Number of null values in 'MGMT status':\", null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'filtered_output.csv'  \n",
    "output_file = 'filtered_output_with_no_non_existant_paths.csv'  \n",
    "filter_non_existent_paths(csv_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"filtered_output_with_no_non_existant_paths.csv\")\n",
    "# display(train_df)\n",
    "\n",
    "df_train, df_valid = sk_model_selection.train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.2, \n",
    "    random_state=12, \n",
    "    stratify=train_df[\"MGMT status\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'MGMT status': [0 1]\n"
     ]
    }
   ],
   "source": [
    "unique_values = train_df['MGMT status'].unique()\n",
    "print(\"Unique values in 'MGMT status':\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mri_types = ['_FLAIR','_T1','_T1c','_T2', 'all']\n",
    "mri_types = ['_FLAIR']\n",
    "SIZE = 256\n",
    "NUM_IMAGES = 64\n",
    "\n",
    "def load_nii_image(path, img_size=(128, 128, 128), rotate=0):\n",
    "    nii = nib.load(path)\n",
    "    data = nii.get_fdata()\n",
    "\n",
    "    if rotate > 0:\n",
    "        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n",
    "        if len(data.shape) == 3:\n",
    "            data = np.array([cv2.rotate(slice, rot_choices[rotate]) for slice in data])\n",
    "        else:\n",
    "            data = cv2.rotate(data, rot_choices[rotate])\n",
    "    \n",
    "    if len(data.shape) == 3:\n",
    "        data = np.array([cv2.resize(slice, (img_size[1], img_size[2]), interpolation=cv2.INTER_AREA) for slice in data])\n",
    "    else:\n",
    "        data = cv2.resize(data, (img_size[1], img_size[2]), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Resize depth if necessary\n",
    "    if data.shape[0] != img_size[0]:\n",
    "        new_data = np.zeros((img_size[0], img_size[1], img_size[2]))\n",
    "        depth = min(img_size[0], data.shape[0])\n",
    "        new_data[:depth] = data[:depth]\n",
    "        data = new_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_nii_images_3d(path, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"_FLAIR\", split=\"train\", rotate=0):\n",
    "    # Check if the path exists\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "        return None  # or handle the non-existent path as needed\n",
    "\n",
    "    files = sorted(glob.glob(path))\n",
    "    # print(\"Files found:\", files)\n",
    "\n",
    "    middle = len(files) // 2\n",
    "    num_imgs2 = num_imgs // 2\n",
    "    p1 = max(0, middle - num_imgs2)\n",
    "    p2 = min(len(files), middle + num_imgs2)\n",
    "\n",
    "    if not files[p1:p2]:\n",
    "        raise ValueError(f\"No files to process in the specified range. Path: {path}\")\n",
    "\n",
    "    # img3d = np.stack([load_nii_image(print(f\"Processing file: {f}\") or f, img_size=img_size, rotate=rotate) for f in files[p1:p2]])\n",
    "    img3d = np.stack([load_nii_image(f, img_size=(128, 128, 128), rotate=rotate) for f in files[p1:p2]])\n",
    "\n",
    "\n",
    "    if img3d.shape[-1] < num_imgs:\n",
    "        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n",
    "        img3d = np.concatenate((img3d, n_zero), axis=-1)\n",
    "        \n",
    "    if np.min(img3d) < np.max(img3d):\n",
    "        img3d = img3d - np.min(img3d)\n",
    "        img3d = img3d / np.max(img3d)\n",
    "            \n",
    "    return np.expand_dims(img3d, 0)\n",
    "\n",
    "class Dataset(torch_data.Dataset):\n",
    "    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\", augment=False, img_size=SIZE, num_imgs=NUM_IMAGES):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.mri_type = mri_type\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "        self.num_imgs = num_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def get_ids(self, index):\n",
    "        # This method returns the corrected ID for a given index\n",
    "        original_path = self.paths[index]\n",
    "        id_num = str(original_path).split('-')[-1] # This splits the ID and takes the last part, which is the number\n",
    "        padded_id_num = id_num.zfill(4)  # Zero padding to ensure 4 digits\n",
    "        corrected_id = f\"UCSF-PDGM-{padded_id_num}\"\n",
    "        \n",
    "#         corrected_path = f\"/home/mm510/data/UCSF-PDGM-v3/UCSF-PDGM-{padded_id_num}_nifti/UCSF-PDGM-{padded_id_num}_FLAIR.nii\"\n",
    "        corrected_path = f\"/data/UCSF-PDGM-{padded_id_num}_nifti/UCSF-PDGM-{padded_id_num}_FLAIR.nii\"\n",
    "        \n",
    "        if not os.path.exists(corrected_path):\n",
    "            print(f\"Path does not exist for ID {corrected_id}: {corrected_path}. Skipping.\")\n",
    "            return None  # Return None for non-existent paths\n",
    "        \n",
    "        return corrected_id\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Original path from the dataset\n",
    "        original_path = self.paths[index]\n",
    "        # print(\"Original path in dataset used:\", original_path)\n",
    "\n",
    "        # Correcting the path as per requirement\n",
    "        # Assuming 'UCSF-PDGM-36' needs to be converted to '/home/mm510/data/UCSF-PDGM-v3/UCSF-PDGM-0366_nifti/UCSF-PDGM-0366_FLAIR.nii'\n",
    "        # Extract the numerical ID part and add zero padding\n",
    "        id_num = str(original_path).split('-')[-1]  # This splits the ID and takes the last part, which is the number\n",
    "        padded_id_num = id_num.zfill(4)  # Zero padding to ensure 4 digits\n",
    "#         corrected_path = f\"/home/mm510/data/UCSF-PDGM-v3/UCSF-PDGM-{padded_id_num}_nifti/UCSF-PDGM-{padded_id_num}_FLAIR.nii\"\n",
    "        corrected_path = f\"/data/UCSF-PDGM-{padded_id_num}_nifti/UCSF-PDGM-{padded_id_num}_FLAIR.nii\"\n",
    "\n",
    "        # Constructing the corrected ID\n",
    "        corrected_id = f\"UCSF-PDGM-{padded_id_num}\"\n",
    "\n",
    "        if not os.path.exists(corrected_path):\n",
    "            print(f\"Path does not exist: {corrected_path}. Skipping.\")\n",
    "            return None  # Return None for non-existent paths\n",
    "\n",
    "        # print(\"Corrected path used:\", corrected_path)\n",
    "        # print(\"Corrected ID:\", corrected_id)\n",
    "\n",
    "        # Augmentation logic remains unchanged\n",
    "        if self.augment:\n",
    "            rotation = np.random.randint(0, 4)\n",
    "        else:\n",
    "            rotation = 0\n",
    "\n",
    "        # Assuming 'load_nii_images_3d' can handle the corrected path format\n",
    "        data = load_nii_images_3d(corrected_path, num_imgs=self.num_imgs, img_size=self.img_size, mri_type=self.mri_type, split=self.split, rotate=rotation)\n",
    "\n",
    "        # Return logic based on whether targets are provided\n",
    "        if self.targets is None:\n",
    "            return {\"X\": torch.tensor(data).float(), \"id\": corrected_id}\n",
    "        else:\n",
    "            y = torch.tensor(abs(self.targets[index] - self.label_smoothing), dtype=torch.float)\n",
    "            return {\"X\": torch.tensor(data).float(), \"y\": y, \"id\": corrected_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        mlp_dim = 2048\n",
    "        for _ in range(depth):\n",
    "            #print (dim, mlp_dim)\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # def __init__(self, *, image_size=(64, 96, 96), patch_size=(8, 8, 8), num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0., emb_dropout=0.):\n",
    "    #     super().__init__()\n",
    "    #     assert all(i % p == 0 for i, p in zip(image_size, patch_size)), 'Image dimensions must be divisible by the patch size'\n",
    "    #     # num_patches = (image_size // patch_size) *(image_size // patch_size)* 2\n",
    "    #     # num_patches = 3840\n",
    "    #     # print(\"patches: \", num_patches)\n",
    "    #     # patch_dim = channels * patch_size ** 3\n",
    "    #     num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1]) * (image_size[2] // patch_size[2])\n",
    "    #     patch_dim = channels * patch_size[0] * patch_size[1] * patch_size[2]\n",
    "    #     self.patch_size = patch_size\n",
    "\n",
    "    #     self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "    #     # print(\"embeddings: \", self.pos_embedding)\n",
    "    #     self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "    #     # print(\"patch embeddings: \", self.patch_to_embedding)\n",
    "    #     self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "    #     self.dropout = nn.Dropout(emb_dropout)\n",
    "    #     #print (mlp_dim)\n",
    "    #     self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n",
    "    #     #print (dim)\n",
    "    #     self.to_cls_token = nn.Identity()\n",
    "\n",
    "    #     self.mlp_head = nn.Sequential(\n",
    "    #         nn.LayerNorm(dim),\n",
    "    #         nn.Linear(dim, mlp_dim),\n",
    "    #         nn.GELU(),\n",
    "    #         nn.Dropout(dropout),\n",
    "    #         nn.Linear(mlp_dim, num_classes),\n",
    "    #         nn.Dropout(dropout)\n",
    "    #     )\n",
    "\n",
    "    def __init__(self, *, image_size=(128, 128, 128), patch_size=(8, 8, 8), num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        # Ensure image_size and patch_size are tuples/lists and have the same length\n",
    "        assert isinstance(image_size, (tuple, list)) and isinstance(patch_size, (tuple, list)), \"image_size and patch_size must be tuples or lists\"\n",
    "        assert len(image_size) == len(patch_size), \"image_size and patch_size must have the same length\"\n",
    "        \n",
    "        # Calculate num_patches by dividing each dimension of image_size by the corresponding dimension of patch_size\n",
    "        num_patches = 1\n",
    "        for img_dim, patch_dim in zip(image_size, patch_size):\n",
    "            assert img_dim % patch_dim == 0, f\"Image dimension {img_dim} is not divisible by patch size {patch_dim}\"\n",
    "            num_patches *= (img_dim // patch_dim)\n",
    "        \n",
    "        patch_dim = channels * patch_size[0] * patch_size[1] * patch_size[2]\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        # print(\"pos embeddings: \", self.pos_embedding)\n",
    "        # self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.patch_to_embedding = nn.Linear(512, dim)\n",
    "        # print(\"patch embeddings: \", self.patch_to_embedding)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, num_classes),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    # def forward(self, img, mask=None):\n",
    "    #     p = self.patch_size\n",
    "\n",
    "    #     # Squeeze the extra singleton dimension if present\n",
    "    #     # Check if there's an extra dimension at position 2 (0-indexed) and squeeze it\n",
    "    #     if img.size(1) == 1 and len(img.shape) == 6:  # Adjust this condition based on where the extra dimension might be\n",
    "    #         img = img.squeeze(1)  # This changes the shape from [4, 1, 1, 240, 256, 256] to [4, 1, 240, 256, 256]\n",
    "\n",
    "    #     # Now proceed with your original operation\n",
    "    #     x = rearrange(img, 'b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=p, p2=p, p3=p)\n",
    "\n",
    "    #     # The rest of your forward method remains unchanged\n",
    "    #     x = self.patch_to_embedding(x)\n",
    "    #     cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "    #     x = torch.cat((cls_tokens, x), dim=1)\n",
    "    #     # print(x.shape)\n",
    "    #     # print(self.pos_embedding.shape)\n",
    "    #     x += self.pos_embedding\n",
    "    #     x = self.dropout(x)\n",
    "    #     x = self.transformer(x)\n",
    "    #     x = self.to_cls_token(x[:, 0])\n",
    "    #     return self.mlp_head(x)\n",
    "        \n",
    "    def forward(self, img, mask=None):\n",
    "        p = self.patch_size\n",
    "\n",
    "        if img.size(1) == 1 and len(img.shape) == 6:  \n",
    "             img = img.squeeze(1) \n",
    "\n",
    "\n",
    "        # expected_shape = (batch_size, channels, depth, height, width) # Adjust as necessary\n",
    "        # assert img.shape == expected_shape, f\"Unexpected img shape: {img.shape}\"\n",
    "        \n",
    "\n",
    "        # Adjust the rearrange pattern according to the actual dimensions\n",
    "        # print(\"Shape before rearrange:\", img.shape)\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=p[0], p2=p[1], p3=p[2])\n",
    "        # print(\"Shape after rearrange:\", x.shape)\n",
    "\n",
    "        x = self.patch_to_embedding(x)\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def init_csv_logger(file_path, headers):\n",
    "    # Check if the log file already exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    \n",
    "    # Open the file in append mode\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        \n",
    "        # Write the header only if the file is new\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.best_valid_score = np.inf\n",
    "        self.n_patience = 0\n",
    "        self.lastmodel = None\n",
    "        \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path, patience, log_file, mri_type):   \n",
    "        headers = [\"epoch\", \"train_loss\", \"valid_loss\", \"valid_auc\", \"train_time\", \"valid_time\", \"mri_type\"]\n",
    "        init_csv_logger(log_file, headers)\n",
    "\n",
    "        for n_epoch in range(1, epochs + 1):\n",
    "            self.info_message(\"EPOCH: {}\", n_epoch)\n",
    "            \n",
    "            train_loss, train_time = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_auc, valid_time = self.valid_epoch(valid_loader)\n",
    "            \n",
    "            self.info_message(\n",
    "                \"[Epoch Train: {}] loss: {:.4f}, time: {:.2f} s            \",\n",
    "                n_epoch, train_loss, train_time\n",
    "            )\n",
    "            \n",
    "            self.info_message(\n",
    "                \"[Epoch Valid: {}] loss: {:.4f}, auc: {:.4f}, time: {:.2f} s\",\n",
    "                n_epoch, valid_loss, valid_auc, valid_time\n",
    "            )\n",
    "\n",
    "            with open(log_file, 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "                writer.writerow({\n",
    "                    \"epoch\": n_epoch,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"valid_loss\": valid_loss,\n",
    "                    \"valid_auc\": valid_auc,\n",
    "                    \"train_time\": train_time,\n",
    "                    \"valid_time\": valid_time,\n",
    "                    \"mri_type\": mri_type\n",
    "                })\n",
    "\n",
    "            # if True:\n",
    "            #if self.best_valid_score < valid_auc: \n",
    "            if self.best_valid_score > valid_loss: \n",
    "                self.save_model(n_epoch, save_path, valid_loss, valid_auc)\n",
    "                self.info_message(\n",
    "                     \"auc improved from {:.4f} to {:.4f}. Saved model to '{}'\", \n",
    "                    self.best_valid_score, valid_loss, self.lastmodel\n",
    "                )\n",
    "                self.best_valid_score = valid_loss\n",
    "                self.n_patience = 0\n",
    "            else:\n",
    "                self.n_patience += 1\n",
    "            \n",
    "            if self.n_patience >= patience:\n",
    "                self.info_message(\"\\nValid auc didn't improve last {} epochs.\", patience)\n",
    "                break\n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        t = time.time()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            X = batch[\"X\"].to(self.device)\n",
    "            targets = batch[\"y\"].to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X).squeeze(1)\n",
    "            \n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            sum_loss += loss.detach().item()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            message = 'Train Step {}/{}, train_loss: {:.4f}'\n",
    "            self.info_message(message, step, len(train_loader), sum_loss/step, end=\"\\r\")\n",
    "        \n",
    "        return sum_loss/len(train_loader), int(time.time() - t)\n",
    "    \n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()\n",
    "        t = time.time()\n",
    "        sum_loss = 0\n",
    "        y_all = []\n",
    "        outputs_all = []\n",
    "\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[\"X\"].to(self.device)\n",
    "                targets = batch[\"y\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(X).squeeze(1)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                sum_loss += loss.detach().item()\n",
    "                y_all.extend(batch[\"y\"].tolist())\n",
    "                outputs_all.extend(torch.sigmoid(outputs).tolist())\n",
    "\n",
    "            message = 'Valid Step {}/{}, valid_loss: {:.4f}'\n",
    "            self.info_message(message, step, len(valid_loader), sum_loss/step, end=\"\\r\")\n",
    "            \n",
    "        y_all = [1 if x > 0.5 else 0 for x in y_all]\n",
    "        auc = roc_auc_score(y_all, outputs_all)\n",
    "        \n",
    "        return sum_loss/len(valid_loader), auc, int(time.time() - t)\n",
    "    \n",
    "    def save_model(self, n_epoch, save_path, loss, auc):\n",
    "        self.lastmodel = f\"{save_path}-best.pth\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "            },\n",
    "            self.lastmodel,\n",
    "        )\n",
    "    \n",
    "    def load_pretrained_model(self, state_path):\n",
    "        checkpoint = torch.load(state_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.best_valid_score = checkpoint[\"best_valid_score\"]\n",
    "        n_epoch = checkpoint[\"n_epoch\"]\n",
    "        self.info_message(\"Loaded pretrained model from '{}', starting from epoch {}.\", state_path, n_epoch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_message(message, *args, end=\"\\n\"):\n",
    "        print(message.format(*args), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for MRI Type: _FLAIR\n",
      "(327, 17) (82, 17)\n",
      "EPOCH: 1\n",
      "[Epoch Train: 1] loss: 0.6981, time: 232.00 s            \n",
      "[Epoch Valid: 1] loss: 0.5865, auc: 0.5182, time: 28.00 s\n",
      "auc improved from inf to 0.5865. Saved model to '_FLAIR-best.pth'\n",
      "['_FLAIR-best.pth']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_mri_type(df_train, df_valid, mri_type):\n",
    "    if mri_type==\"all\":\n",
    "        train_list = []\n",
    "        valid_list = []\n",
    "        for mri_type in mri_types:\n",
    "            df_train.loc[:,\"MRI_Type\"] = mri_type\n",
    "            train_list.append(df_train.copy())\n",
    "            df_valid.loc[:,\"MRI_Type\"] = mri_type\n",
    "            valid_list.append(df_valid.copy())\n",
    "\n",
    "        df_train = pd.concat(train_list)\n",
    "        df_valid = pd.concat(valid_list)\n",
    "    else:\n",
    "        df_train.loc[:,\"MRI_Type\"] = mri_type\n",
    "        df_valid.loc[:,\"MRI_Type\"] = mri_type\n",
    "\n",
    "    print(f\"Training for MRI Type: {mri_type}\")\n",
    "    print(df_train.shape, df_valid.shape)\n",
    "    # display(df_train.head())\n",
    "    \n",
    "    train_data_retriever = Dataset(\n",
    "        df_train[\"ID\"].values, \n",
    "        df_train[\"MGMT status\"].values, \n",
    "        df_train[\"MRI_Type\"].values,\n",
    "        augment=True\n",
    "    )\n",
    "\n",
    "    valid_data_retriever = Dataset(\n",
    "        df_valid[\"ID\"].values, \n",
    "        df_valid[\"MGMT status\"].values,\n",
    "        df_valid[\"MRI_Type\"].values\n",
    "    )\n",
    "\n",
    "    train_loader = torch_data.DataLoader(\n",
    "        train_data_retriever,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=8,pin_memory = True\n",
    "    )\n",
    "\n",
    "    valid_loader = torch_data.DataLoader(\n",
    "        valid_data_retriever, \n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=8,pin_memory = True\n",
    "    )\n",
    "\n",
    "    model = Model(\n",
    "    image_size=(128, 128, 128),\n",
    "    patch_size=(8, 8, 8),\n",
    "    num_classes=1,\n",
    "    dim=512,\n",
    "    depth=2,\n",
    "    heads=8,\n",
    "    mlp_dim=1024,\n",
    "    channels=3,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "#     checkpoint = torch.load(\"all-best.pth\")\n",
    "#     model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    #print(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    log_file_path = \"transformer_training_log.csv\"\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion\n",
    "    )\n",
    "\n",
    "    history = trainer.fit(\n",
    "        1, \n",
    "        train_loader, \n",
    "        valid_loader, \n",
    "        f\"{mri_type}\", \n",
    "        10,\n",
    "        log_file=log_file_path,\n",
    "        mri_type=mri_type\n",
    "    )\n",
    "    \n",
    "    return trainer.lastmodel\n",
    "#     return model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "modelfiles = None\n",
    "\n",
    "if not modelfiles:\n",
    "    modelfiles = [train_mri_type(df_train, df_valid, m) for m in mri_types]\n",
    "    print(modelfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelfile, df, mri_type, split):\n",
    "    print(\"Predict:\", modelfile, mri_type, df.shape)\n",
    "    # df.loc[:,\"MRI_Type\"] = mri_type\n",
    "    # print(\"these are indexes: \")\n",
    "    # print(\"this is the data: \")\n",
    "    # df.columns\n",
    "    # print(df.index.values)\n",
    "    data_retriever = Dataset(\n",
    "        df.index.values, \n",
    "        mri_type=df[\"MRI_Type\"].values,\n",
    "        split=split\n",
    "    )\n",
    "\n",
    "    data_loader = torch_data.DataLoader(\n",
    "        data_retriever,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "    )\n",
    "   \n",
    "    model = Model(\n",
    "        image_size=(128, 128, 128),\n",
    "        patch_size=(8, 8, 8),\n",
    "        num_classes=1,\n",
    "        dim=512,\n",
    "        depth=2,\n",
    "        heads=8,\n",
    "        mlp_dim=1024,\n",
    "        channels=3,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint = torch.load(modelfile)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = []\n",
    "    ids = []\n",
    "\n",
    "    for e, batch in enumerate(data_loader, 1):\n",
    "        print(f\"{e}/{len(data_loader)}\", end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n",
    "            if tmp_pred.size == 1:\n",
    "                y_pred.append(float(tmp_pred))\n",
    "            else:\n",
    "                y_pred.extend(tmp_pred.tolist())\n",
    "            ids.extend(batch[\"id\"])  # Ensure this is correctly creating a flat list\n",
    "            \n",
    "    preddf = pd.DataFrame({\"MGMT status\": y_pred}, index=ids)  # Directly use ids as index\n",
    "\n",
    "    return preddf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: _FLAIR-best.pth _FLAIR (82, 16)\n",
      "unique values in df_valid  [1 0]\n",
      "               MGMT status\n",
      "ID                        \n",
      "UCSF-PDGM-038     0.738747\n",
      "UCSF-PDGM-358     0.738716\n",
      "UCSF-PDGM-360     0.738330\n",
      "UCSF-PDGM-017     0.738584\n",
      "UCSF-PDGM-538     0.738152\n",
      "NaN in 'MGMT_pred': 0\n",
      "              Sex  Age at MRI  WHO CNS Grade  \\\n",
      "ID                                             \n",
      "UCSF-PDGM-038   M          78              4   \n",
      "UCSF-PDGM-358   M          57              4   \n",
      "UCSF-PDGM-360   F          62              4   \n",
      "UCSF-PDGM-017   M          54              4   \n",
      "UCSF-PDGM-538   F          41              4   \n",
      "\n",
      "              Final pathologic diagnosis (WHO 2021)  MGMT status  MGMT index  \\\n",
      "ID                                                                             \n",
      "UCSF-PDGM-038            Glioblastoma, IDH-wildtype          1.0        16.0   \n",
      "UCSF-PDGM-358            Glioblastoma, IDH-wildtype          1.0        14.0   \n",
      "UCSF-PDGM-360            Glioblastoma, IDH-wildtype          1.0        15.0   \n",
      "UCSF-PDGM-017            Glioblastoma, IDH-wildtype          0.0         0.0   \n",
      "UCSF-PDGM-538            Glioblastoma, IDH-wildtype          0.0         0.0   \n",
      "\n",
      "               1p/19q       IDH  1-dead 0-alive     OS  EOR  \\\n",
      "ID                                                            \n",
      "UCSF-PDGM-038     NaN  wildtype               1  152.0  STR   \n",
      "UCSF-PDGM-358  intact  wildtype               1  430.0  STR   \n",
      "UCSF-PDGM-360  intact  wildtype               0  777.0  GTR   \n",
      "UCSF-PDGM-017     NaN  wildtype               1  241.0  STR   \n",
      "UCSF-PDGM-538  intact  wildtype               0  287.0  GTR   \n",
      "\n",
      "              Biopsy prior to imaging       BraTS21 ID  \\\n",
      "ID                                                       \n",
      "UCSF-PDGM-038                      No  BraTS2021_00082   \n",
      "UCSF-PDGM-358                     Yes  BraTS2021_00712   \n",
      "UCSF-PDGM-360                      No  BraTS2021_00689   \n",
      "UCSF-PDGM-017                      No  BraTS2021_00003   \n",
      "UCSF-PDGM-538                      No              NaN   \n",
      "\n",
      "              BraTS21 Segmentation Cohort BraTS21 MGMT Cohort MRI_Type  \\\n",
      "ID                                                                       \n",
      "UCSF-PDGM-038                  Validation          Validation   _FLAIR   \n",
      "UCSF-PDGM-358                  Validation          Validation   _FLAIR   \n",
      "UCSF-PDGM-360                    Training                 NaN   _FLAIR   \n",
      "UCSF-PDGM-017                    Training            Training   _FLAIR   \n",
      "UCSF-PDGM-538                         NaN                 NaN   _FLAIR   \n",
      "\n",
      "               MGMT_pred  \n",
      "ID                        \n",
      "UCSF-PDGM-038   0.738747  \n",
      "UCSF-PDGM-358   0.738716  \n",
      "UCSF-PDGM-360   0.738330  \n",
      "UCSF-PDGM-017   0.738584  \n",
      "UCSF-PDGM-538   0.738152  \n",
      "Validation ensemble AUC: 0.5182\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_valid = df_valid.set_index(\"ID\")\n",
    "# df_valid['ID']\n",
    "\n",
    "\n",
    "true_values_column_name = \"MGMT status\" \n",
    "\n",
    "# df_valid[\"MGMT_pred\"] = 0\n",
    "# for m, mtype in zip(modelfiles,  mri_types):\n",
    "#     pred = predict(m, df_valid, mtype, \"train\")\n",
    "#     df_valid[\"MGMT_pred\"] += pred[\"MGMT status\"]  \n",
    "# df_valid[\"MGMT_pred\"] /= len(modelfiles)\n",
    "\n",
    "for m, mtype in zip(modelfiles, mri_types):\n",
    "    pred = predict(m, df_valid, mtype, \"train\")\n",
    "    if \"MGMT_pred\" not in df_valid.columns:\n",
    "        df_valid[\"MGMT_pred\"] = 0  # Initialize if not exists\n",
    "    df_valid[\"MGMT_pred\"] += pred[\"MGMT status\"].reindex(df_valid.index).fillna(0)\n",
    "\n",
    "pred.index = df_valid.index \n",
    "df_valid[\"MGMT_pred\"] = pred[\"MGMT status\"]\n",
    "df_valid[\"MGMT_pred\"] /= len(modelfiles)\n",
    "print(\"unique values in df_valid \", df_valid[\"MGMT status\"].unique())\n",
    "\n",
    "\n",
    "\n",
    "print(pred.head())\n",
    "print(\"NaN in 'MGMT_pred':\", df_valid[\"MGMT_pred\"].isna().sum())\n",
    "\n",
    "df_valid[\"MGMT status\"] = df_valid[\"MGMT status\"].astype(float)\n",
    "df_valid[\"MGMT_pred\"] = df_valid[\"MGMT_pred\"].astype(float)\n",
    "\n",
    "print(df_valid.head())\n",
    "\n",
    "auc = roc_auc_score(df_valid[\"MGMT status\"], df_valid[\"MGMT_pred\"])\n",
    "print(f\"Validation ensemble AUC: {auc:.4f}\")\n",
    "\n",
    "\n",
    "# if true_values_column_name in df_valid:\n",
    "\n",
    "#     unique_values = df_valid[\"MGMT_pred\"].unique()\n",
    "#     print(\"Unique values in 'MGMT_pred':\", unique_values)\n",
    "#     auc = roc_auc_score(df_valid[true_values_column_name], df_valid[\"MGMT_pred\"])  \n",
    "#     print(f\"Validation ensemble AUC: {auc:.4f}\")\n",
    "# else:\n",
    "#     print(f\"Column '{true_values_column_name}' not found in df_valid.\")\n",
    "g = sns.displot(df_valid[\"MGMT_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Given data for the models\n",
    "data = {\n",
    "    \"Models\": [\n",
    "        \"Hybrid Loss 1\", \"Hybrid Loss 1\", \"Hybrid Loss 1\",\n",
    "        \"Hybrid Loss 2\", \"Hybrid Loss 2\", \"Hybrid Loss 2\",\n",
    "        \"Custom Dice Loss\", \"Custom Dice Loss\", \"Custom Dice Loss\"\n",
    "    ],\n",
    "    \"Metrics\": [\n",
    "        \"Necrotic\", \"Edema\", \"Enhancing\",\n",
    "        \"Necrotic\", \"Edema\", \"Enhancing\",\n",
    "        \"Necrotic\", \"Edema\", \"Enhancing\"\n",
    "    ],\n",
    "    \"Values\": [\n",
    "        0.9161, 0.8926, 0.9039,\n",
    "        0.432, 0.4231, 0.4121,\n",
    "        0.762, 0.763, 0.738\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to format suitable for the desired plot\n",
    "# Note that we now use keyword arguments instead of positional arguments.\n",
    "df_pivot = df.pivot(index='Models', columns='Metrics', values='Values')\n",
    "df_pivot = df_pivot.reindex([\"Necrotic\", \"Edema\", \"Enhancing\"], axis=1)\n",
    "\n",
    "# Start plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create a color map to match the original graph's color scheme\n",
    "colors = [\"#7E57C2\", \"#FFCA28\", \"#26A69A\"]\n",
    "\n",
    "# Plot the bars\n",
    "df_pivot.plot(kind='bar', ax=ax, color=colors)\n",
    "\n",
    "# Set the x-axis label\n",
    "ax.set_xlabel(\"Models\", fontsize=12)\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel(\"Dice Coefficients\", fontsize=12)\n",
    "\n",
    "# Set the title of the plot\n",
    "ax.set_title(\"Summary of Final Results\", fontsize=16)\n",
    "\n",
    "# Position the legend outside of the plot\n",
    "ax.legend(title=\"Target Class Key\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Customize the grid lines\n",
    "ax.grid(True, which='both', ls='--', lw=.5, color='gray', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
